{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../lib')\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../src/queries')\n",
    "sys.path.append('../src/clustering')\n",
    "sys.path.append('../src/visualization_lib')\n",
    "sys.path.append('../data')\n",
    "sys.path.append('../')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from lib import csv_reader as reader\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from lib import year_classifier as year_classifier\n",
    "\n",
    "from jinja2 import Environment, FileSystemLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gen html\n",
    "# http://pbpython.com/pdf-reports.html\n",
    "\n",
    "#reports folder\n",
    "dir_reports = '../AnalysisTool/'\n",
    "fsl = FileSystemLoader(dir_reports)\n",
    "env = Environment(loader=fsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import medfilt\n",
    "from scipy.signal import convolve\n",
    "\n",
    "def append_mid_year(df, column='mid_year'):\n",
    "    '''\n",
    "    Adds a int column (default \"mid_year\") to the table. It is the mean (rounded up) of from_year and to_year.\n",
    "    '''\n",
    "    df[column] = (df['from_year']+df['to_year'])/2\n",
    "    df[column] = (df['mid_year']+0.49).astype(int)\n",
    "    return df\n",
    "  \n",
    "\n",
    "def plotTopic(topicsListsWithIDs, df, column='mid_year', smooth=True):\n",
    "    '''\n",
    "    topicsListsWithIDs takes a List with Lists of Topic-Cluster-IDs \n",
    "    like: [[52, 67, 85, 96],[62]]\n",
    "    '''\n",
    "    df = df.copy(deep=True)\n",
    "    the_title = \"Topics:\"+str(topicsListsWithIDs)+' '+ column\n",
    "\n",
    "    res= []\n",
    "    for topic_ids in topicsListsWithIDs:\n",
    "        topic_df = df[df['cluster_id'].isin(topic_ids)]\n",
    "\n",
    "        #res_df = topic_df.groupby([df[column]]).count().add_suffix('_count').reset_index()[[column, 'cluster_id_count']]\n",
    "\n",
    "        res_df = topic_df[[column, 'picture_id', 'cluster_id']].groupby([column, 'picture_id']).count().add_suffix('_count').reset_index()\n",
    "\n",
    "        # problem topicsListsWithIDs = [[52, 67, 85, 96],[62]]\n",
    "        # die erste topicslist hat mehr Einträge. Da diese ähnliche Cluster sind ist die warhscheinlichkeit hoch, \n",
    "        # dass ein Bild diese Ids aus als cluster enthält und somit wird das Bild öffter gezählt\n",
    "        # Lösung:\n",
    "        res_df = res_df[[column, 'picture_id']].groupby(column).count().add_suffix('_count').reset_index()\n",
    "        \n",
    "        \n",
    "        #normalize\n",
    "        df_all = df[[column, 'picture_id']].groupby(column).count().add_suffix('_count').reset_index()\n",
    "        df_all['all_picture_id_count'] = df_all['picture_id_count'].map(lambda x: x/4)\n",
    "\n",
    "        res_df = pd.merge(res_df, df_all[[column, 'all_picture_id_count']], on=column)\n",
    "        \n",
    "        \n",
    "        #res_df['picture_id_count_normalized'] = res_df.map(lambda x: float(x['picture_id_count']) / x['all_picture_id_count'])\n",
    "        res_df['picture_id_count_normalized'] = res_df['picture_id_count'] / res_df['all_picture_id_count']\n",
    "        \n",
    "\n",
    "        # smoothing\n",
    "        #f = interp1d(test_x, medfilt(test_y, 7), kind='cubic')\n",
    "        #xnew = np.linspace(1785, 1918, 20)\n",
    "        #xnew, f(xnew), 'g-', \n",
    "        k2 = [0.5,0.5]\n",
    "        k4 = [0.25,0.25,0.25,0.25]\n",
    "        k5 = [0.2,0.2,0.2,0.2,0.2]\n",
    "        k10 = [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1]\n",
    "        \n",
    "        if smooth:\n",
    "            res_df['picture_id_count_normalized'] = convolve(res_df['picture_id_count_normalized'], k4, mode='same')\n",
    "                      \n",
    "        legendname = 'Topic-Clusters:'+str(topic_ids)\n",
    "        res_df.rename(columns={'picture_id_count_normalized': legendname}, inplace=True)\n",
    "        res.append(res_df[[column,legendname]])\n",
    "\n",
    "    # join them\n",
    "    plot_df = pd.concat(res)\n",
    "    #the_title = \"Topics:\"+str(topicIDs)+' '+ column\n",
    "    \n",
    "    data_y = np.array(plot_df[legendname])\n",
    "    data_x = np.array(plot_df[column])\n",
    "    \n",
    "    fig = plt.figure(figsize=(11, 7))\n",
    "    plt.title(the_title)\n",
    "    #plt.plot(x=column, xticks=range(1785,1918,10), xlim=((1785,1918)), figsize=(11,5))\n",
    "    plt.legend(loc='best')\n",
    "    plt.plot(data_x, data_y)\n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "    \n",
    "\n",
    "def plotAllCount(df_o, column='mid_year'):   \n",
    "    df = df_o.copy(deep=True)\n",
    "   \n",
    "    df = df[[column, 'picture_id']].groupby([column]).count().add_suffix('_count').reset_index()\n",
    "    df['picture_id_count'] = df['picture_id_count'].map(lambda x: x/4)\n",
    "    \n",
    "    the_title = 'All Count '+column\n",
    "    \n",
    "    \n",
    "    data_y = np.array(df['picture_id_count'])\n",
    "    data_x = np.array(df[column])\n",
    "    \n",
    "    fig = plt.figure(figsize=(11, 5))\n",
    "    plt.title(the_title)\n",
    "    #plot = df[[column, 'picture_id_count']].plot(x=column, xticks=range(1785,1918,10), xlim=((1785,1918)), title=the_title, figsize=(11,5), legend=False)\n",
    "    \n",
    "    #, xticks=range(1785,1918,10)\n",
    "    #, xlim=((1785,1918))\n",
    "    plt.plot(data_x, data_y)\n",
    "    fig.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "def plotMatrix(df_o):\n",
    "    # Display a random matrix with a specified figure number and a grayscale\n",
    "    # colormap\n",
    "    #plt.clf() \n",
    "    \n",
    "    #df = df_o.copy(deep=True)\n",
    "    \n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    plt.title('Topic Matrix')\n",
    "    plt.matshow(topic_martix_df.as_matrix(), fignum=100, cmap=plt.cm.gray)\n",
    "\n",
    "    topic_martix_path = dir_reports+\"/plots/\"+\"topic_martix.jpg\"\n",
    "    #plt.show()\n",
    "\n",
    "    return plt.gcf()\n",
    "\n",
    "def plotTopicFrequently(df_o):\n",
    "    df = df_o.copy(deep=True)\n",
    "    n = 10\n",
    "    \n",
    "    df = df.sort_values('count', ascending=False)[0:n]\n",
    "    topics= []*n\n",
    "    for i, x in enumerate(df['name']):\n",
    "        topics.append( x )\n",
    "    for i, x in enumerate(df['topic_id']):\n",
    "        topics[i] = str(x) + ': ' + str(topics[i])\n",
    "\n",
    "    y_pos = np.arange(len(topics))\n",
    "\n",
    "    count = df['count']\n",
    "\n",
    "    fig = plt.figure(figsize=(11, 4))\n",
    "    plt.barh(y_pos, count)\n",
    "    plt.yticks(y_pos, topics)\n",
    "    #plt.xlabel('Topic')\n",
    "    plt.xlabel('Count')\n",
    "    plt.title('Often combined top ' + str(n))\n",
    "    #plt.margins(0.2)\n",
    "    #plt.subplots_adjust(bottom=0.15)\n",
    "    #plt.set_xticklabels\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merge Data!!!\n",
    "import db\n",
    "my_db = db.Db()\n",
    "\n",
    "# merge Tags\n",
    "meta_tag_120_df = pd.merge(my_db.metadata_long_19, my_db.clusters_long_19)\n",
    "#meta_tag_120_df['clusters_count'] = 4\n",
    "\n",
    "topics_per_picture = 4\n",
    "\n",
    "append_mid_year(meta_tag_120_df);\n",
    "\n",
    "print ('-> merging clusters_long_19 lost %d that is %f p.' % (len(my_db.metadata_long_19) - len(meta_tag_120_df)/4, float(len(meta_tag_120_df)) / len(my_db.metadata_long_19)/4))\n",
    "# pictures with tags only!\n",
    "#meta_tag_120_df\n",
    "\n",
    "# merge artists \n",
    "# on picture_id\n",
    "meta_tag_120_artists_df = pd.merge(meta_tag_120_df, my_db.artist_origin[['picture_id','metadata_nationality','metadata_country','metadata_capital','metadata_longitude', 'metadata_latitude']], on='picture_id')\n",
    "\n",
    "\n",
    "print ('-> merging artists lost %d that is %f p.' % (len(meta_tag_120_df)/topics_per_picture - len(meta_tag_120_artists_df)/topics_per_picture, float(len(meta_tag_120_artists_df)/topics_per_picture) / len(meta_tag_120_df)/topics_per_picture))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_tag_120_artists_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# matrix, topics_frequently_df\n",
    "\n",
    "\n",
    "#u jedem untersuchendem topic ein df\n",
    "# 'topic_id', 'name', 'count'\n",
    "\n",
    "import json\n",
    "with open('../data/topics_benannt.txt') as topic_word_json:\n",
    "        topic_word_dic = json.load(topic_word_json)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "topics = range(0,110)\n",
    "#opics = [3,4]\n",
    "\n",
    "\n",
    "matrixAll = [None] * 110\n",
    "topics_frequently_df = [None]*110\n",
    "\n",
    "for topicA in topics:\n",
    "    #rint(topicA)\n",
    "    \n",
    "    picture_id_with_topic = meta_tag_120_artists_df[meta_tag_120_artists_df['cluster_id']==topicA]['picture_id']\n",
    "    #for topicB in \n",
    "    a = meta_tag_120_artists_df[meta_tag_120_artists_df['cluster_id'] != topicA]\n",
    "\n",
    "    matrix= [0] * 110\n",
    "\n",
    "    for pic_id in picture_id_with_topic:\n",
    "        pictures = a[a['picture_id']== pic_id]\n",
    "        for topic_id in pictures['cluster_id']:\n",
    "            matrix[topic_id] = matrix[topic_id] +1\n",
    "\n",
    "    matrixAll[topicA] = matrix\n",
    "    \n",
    "    all_dfs = []\n",
    "    for i, count in enumerate(matrix):\n",
    "        df = pd.DataFrame([[i, topic_word_dic[str(i)], count]])\n",
    "        df.columns= ['topic_id', 'name', 'count']\n",
    "        all_dfs.append(df)\n",
    "\n",
    "        \n",
    "    # topics_frequently_df\n",
    "    topics_frequently_df[topicA] = all_dfs[topics[0]]\n",
    "    topics_frequently_df[topicA].columns= ['topic_id', 'name', 'count']\n",
    "    for x in all_dfs[1:]:\n",
    "        topics_frequently_df[topicA] = topics_frequently_df[topicA].append(x, ignore_index=True)\n",
    "\n",
    "# results:\n",
    "        \n",
    "# matrix \n",
    "topic_martix_df = pd.DataFrame(matrixAll)\n",
    "\n",
    "# topics_frequently_df\n",
    "topics_frequently_df = topics_frequently_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save\n",
    "\n",
    "f = plotAllCount(meta_tag_120_artists_df);\n",
    "AbsoluteTopicCount_path = dir_reports+\"/plots/\"+\"AbsoluteTopicCount.jpg\"\n",
    "f.savefig(AbsoluteTopicCount_path)\n",
    "\n",
    "\n",
    "f = plotMatrix(topic_martix_df);\n",
    "topic_martix_path = dir_reports+\"/plots/\"+\"topic_martix.jpg\"\n",
    "f.savefig(topic_martix_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for topic in range(0,109):\n",
    "    f = plotTopicFrequently(topics_frequently_df[topic]);\n",
    "    AbsoluteTopics_path = dir_reports+\"/plots/\"+\"TopicFrequentlyPlot_\"+str(topic)+\".jpg\"\n",
    "    f.savefig(AbsoluteTopics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in range(0,109):\n",
    "    topic = x\n",
    "    f = plotTopic([[topic]], meta_tag_120_artists_df);\n",
    "    AbsoluteTopics_path = dir_reports+\"/plots/\"+\"NormalizedTopicPlot_\"+str(topic)+\".jpg\"\n",
    "    f.savefig(AbsoluteTopics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from parallel plot\n",
    "from src.queries import db\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.tools.plotting import parallel_coordinates\n",
    "\n",
    "def compare_cluster_epochs_nationality(clusters=[0],countries=['Germany', 'France','Britain'], epochs=[1789,1848,1875,1914]):\n",
    "    my_db = db.Db()\n",
    "    #combined = pd.merge(my_db.metadata_long_19, my_db.clusters_long_19)\n",
    "    combined = my_db.final_cluster_nation\n",
    "    epochs_window = [epochs[i:i+2] for i in xrange(len(epochs)-1)]\n",
    "\n",
    "    results = []\n",
    "    for nationality in countries:\n",
    "        tmp = [nationality]\n",
    "        for (beginning, end) in epochs_window:\n",
    "            # This has to be renormalized later!\n",
    "            art_per_nation_and_epoch = combined.query('metadata_country == \"{0}\" & mid_year >= {1} & mid_year < {2}'\n",
    "                                                                       .format(nationality, beginning, end))\n",
    "            #count = art_per_nation_and_epoch.groupby('cluster_id').count()\n",
    "            #print count['metadata_surname']\n",
    "            art_per_nation_epoch_and_cluster = art_per_nation_and_epoch[art_per_nation_and_epoch['cluster_id']\n",
    "                .isin(clusters)].groupby('picture_id').count()\n",
    "\n",
    "            #!!!!important step!!!!\n",
    "            tmp.append(len(art_per_nation_epoch_and_cluster)/float(len(art_per_nation_and_epoch)))  \n",
    "        results.append(tmp)\n",
    "    return results\n",
    "\n",
    "def plot_parallel(topic_group, topic_group_name):\n",
    "    #plot\n",
    "    df_plot = pd.DataFrame(compare_cluster_epochs_nationality(topic_group), columns=['country', '1789-1847', '1848-1874', '1875-1914'])\n",
    "\n",
    "    fig = plt.figure(figsize=(11, 4))\n",
    "    plt.ylabel('relative frequency')\n",
    "    plt.xlabel('time range')\n",
    "    plt.title('Topic: {0} {1}: Frequency over Time and by Nationalities'.format(topic_group_name,topic_group))\n",
    "    #plt.suptitle('Cluster description: {0}'. format('to come ...'))\n",
    "    parallel_coordinates(df_plot, 'country', colormap='jet', linewidth=5)\n",
    "\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "for x in range(0,109):\n",
    "    topic = x\n",
    "    #f = plotTopic([[topic]], meta_tag_120_artists_df);\n",
    "    f = plot_parallel([topic], '')\n",
    "    AbsoluteTopics_path = dir_reports+\"/plots/\"+\"Parallel_Plot_\"+str(topic)+\".jpg\"\n",
    "    f.savefig(AbsoluteTopics_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# path\n",
    "my_reader = reader.CSV_reader()\n",
    "paths_df = pd.DataFrame(my_reader.get(my_reader.path_image_path), columns = ['picture_id', 'data_name', 'data_path'])\n",
    "\n",
    "paths_df['picture_id'] = paths_df['picture_id'].astype(int)\n",
    "paths_df['data_name'] = paths_df['data_name'].astype(str)\n",
    "paths_df['data_path'] = paths_df['data_path'].astype(str)\n",
    "\n",
    "#paths_df\n",
    "\n",
    "\n",
    "# cluster words\n",
    "my_reader = reader.CSV_reader()\n",
    "cluster_words = pd.DataFrame(my_reader.get('../data/topics.txt'), columns = ['words'])\n",
    "\n",
    "cluster_words['words'] = cluster_words['words'].astype(str)\n",
    "\n",
    "#cluster_words['words'] = cluster_words['words'].map(lambda x: (\"'\" +x+\"'\").decode('utf-8'))\n",
    "#cluster_words['words'] = cluster_words['words'].map(lambda x: u\"'\" +x)\n",
    "\n",
    "cluster_words['words'] = cluster_words['words'].map(lambda x: x.split(':')[1])\n",
    "#cluster_words['topic'] = cluster_words_tmp['words'].map(lambda x: x.split(':')[0])\n",
    "\n",
    "#cluster_words\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# topic namen\n",
    "\n",
    "with open('../data/topics_benannt.txt') as topic_word_json:\n",
    "        topic_word_dic = json.load(topic_word_json)\n",
    "\n",
    "#topic_word_dic\n",
    "\n",
    "\n",
    "\n",
    "# merge img paths\n",
    "#paths_df.query('picture_id == 100000')\n",
    "df = pd.merge(meta_tag_120_artists_df, paths_df, on='picture_id')\n",
    "\n",
    "df['data_path'] = df['data_path'].map(lambda x:x[11:])\n",
    "\n",
    "df['my_path'] = 'data/artigo-images'+df['data_path']+df['data_name']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "template_topics = []\n",
    "\n",
    "\n",
    "# 0-109\n",
    "cluster_topics = range(0,109)\n",
    "#cluster_topics = [np.random.randint(109)]\n",
    "#cluster_topics = [0,1,2,3,4,5,6]\n",
    "\n",
    "max_num_images = 9\n",
    "rank = 1\n",
    "#folder = '/albertina/'\n",
    "folder = '/koeln/'\n",
    "\n",
    "folders = ['/albertina/','/amherst/','/inspektorx/','/koeln/', '/artemis/','/kunsthalle_karlsruhe/']\n",
    "\n",
    "for cluster_topic in cluster_topics:\n",
    "    #df21 = df[df['data_path'] == folder]\n",
    "    df21 = df[df['data_path'].isin( folders)]\n",
    "    df2 = df21[df21['cluster_id'] == cluster_topic]\n",
    "    \n",
    "    df3 = df2[df2['cluster_rank'] == rank]\n",
    "    #images = np.array(df3['my_path'])\n",
    "    image_ids = np.array(df3['picture_id'])\n",
    "    cluster_ranks = np.array(df3['cluster_rank'])\n",
    "\n",
    "    topic_dic={}\n",
    "    \n",
    "    if(len(image_ids) != 0):\n",
    "\n",
    "        if len(image_ids) >= max_num_images:\n",
    "            image_ids = image_ids[0:len(image_ids)-1]\n",
    "            #np.random.shuffle(images)\n",
    "            image_ids = image_ids[:max_num_images]\n",
    "\n",
    "        words = np.array(cluster_words)[cluster_topic]\n",
    "        \n",
    "        images = []\n",
    "        for picture_id in image_ids:\n",
    "            image_df = df21[df21['picture_id'] == picture_id]\n",
    "            image_df_first = image_df[image_df['cluster_rank'] == 1]\n",
    "            \n",
    "            #print(np.array(image_df_first['metadata_country'])[0])\n",
    "            \n",
    "            image = {\n",
    "                'picture_id': picture_id,\n",
    "                'data_name': np.array(image_df_first['data_name'])[0],\n",
    "                'data_path': np.array(image_df_first['data_path'])[0],\n",
    "                'my_path': np.array(image_df_first['my_path'])[0],\n",
    "                \n",
    "                'mid_year': np.array(image_df_first['mid_year'])[0],\n",
    "                'metadata_country': str(np.array(image_df_first['metadata_country'])[0]).decode('utf-8'),\n",
    "                'metadata_name': str(np.array(image_df_first['metadata_name'])[0]).decode('utf-8'),\n",
    "                'metadata_surname': str(np.array(image_df_first['metadata_surname'])[0]).decode('utf-8'),\n",
    "            }\n",
    "            \n",
    "            topics = []\n",
    "            for topic_id in image_df['cluster_id']:\n",
    "                cluster_rank = image_df[image_df['cluster_id']==topic_id]['cluster_rank']\n",
    "                \n",
    "                topic = {\n",
    "                    'topic_id':topic_id,\n",
    "                    'topic_name':topic_word_dic[str(topic_id)],\n",
    "                    'cluster_rank': int(cluster_rank)\n",
    "                }\n",
    "                topics.append(topic)\n",
    "            \n",
    "            image['topics']= topics\n",
    "            images.append(image)\n",
    "        \n",
    "        topic_dic = {\n",
    "            'topic_id':cluster_topic,\n",
    "            'topic_name': topic_word_dic[str(cluster_topic)],\n",
    "            'words':words,\n",
    "            'images': images\n",
    "        }\n",
    "        \n",
    "        template_topics.append(topic_dic)\n",
    "\n",
    "template_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# copy using images to folder\n",
    "from shutil import copyfile\n",
    "count = 0\n",
    "destination = dir_reports + '/images'\n",
    "\n",
    "# 24 missing\n",
    "for x in range(107):\n",
    "    print x, template_topics[x]['images'][0]['topics'][0]['topic_id']\n",
    "    for img in template_topics[x]['images']:\n",
    "        count+=1\n",
    "        print img['data_path']\n",
    "        copyfile('../'+img['my_path'],destination+'/'+img['data_name'])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Jinja\n",
    "\n",
    "txt_merging_topics = 'Merging clusters_long_19 lost %d rows that is %f p.' % (len(my_db.metadata_long_19) - len(meta_tag_120_df)/4, float(len(meta_tag_120_df)) / len(my_db.metadata_long_19)/4)\n",
    "txt_merging_artists = 'Merging artists lost %d rows that is %f p.' % (len(meta_tag_120_df)/topics_per_picture - len(meta_tag_120_artists_df)/topics_per_picture, float(len(meta_tag_120_artists_df)/topics_per_picture) / len(meta_tag_120_df)/topics_per_picture)\n",
    "\n",
    "\n",
    "template_vars = {\n",
    "    \"title\" : \"Sales Funnel Report - National\",\n",
    "    \"national_pivot_table\": \"test\",\n",
    "    \"AbsoluteTopicCount\": AbsoluteTopicCount_path,\n",
    "    \"topics\": template_topics,\n",
    "    \"PathReportPlots\": '../AnalysisTool/plots/',\n",
    "    \"topic_martrix\": topic_martix_df,\n",
    "    \"txt_merging_topics\": txt_merging_topics,\n",
    "    \"txt_merging_artists\": txt_merging_artists,\n",
    "    \"num_pictures\": len(meta_tag_120_artists_df)/4,\n",
    "    }\n",
    "\n",
    "\n",
    "#env.list_templates()\n",
    "\n",
    "template = env.get_template(\"templates/mytemplate1.html\")\n",
    "\n",
    "#render\n",
    "html_out = template.render(template_vars)\n",
    "#save\n",
    "with open(dir_reports+\"Output.html\", \"w\") as text_file:\n",
    "    text_file.write(html_out.encode('utf-8'))\n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}